{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7892088d-1ea8-48a7-bb4b-efe0d86f414e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zheyu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from scipy.special import expit\n",
    "def process_json_files(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    for filename in os.listdir(input_dir):\n",
    "        input_filepath = os.path.join(input_dir, filename)\n",
    "        if os.path.isfile(input_filepath) and filename.endswith('.json'):\n",
    "            index = filename.split('.')[0].split('part')[-1]  \n",
    "            output_filepath = os.path.join(output_dir, f'geo_tweets_part{index}.json')\n",
    "            \n",
    "            with open(input_filepath, 'r', encoding='utf-8') as infile, open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "                data = json.load(infile) \n",
    "                output_data = [] \n",
    "                for entry in data:\n",
    "                    if 'includes' in entry['doc'].keys():\n",
    "                        new_entry = {\n",
    "                            'id': entry['doc']['_id'],\n",
    "                            'created_at': entry['doc']['data']['created_at'],\n",
    "                            'sentiment': entry['doc']['data'].get('sentiment', None),\n",
    "                            'text': entry['value']['text'],\n",
    "                           file:///C:/Users/zheyu/AppData/Local/Programs/Python/Python312/Lib/site-packages/tqdm/auto.py#line=20 'geo':entry['doc']['includes']['places'][0]['full_name'],\n",
    "                            'coordinates':entry['doc']['includes']['places'][0]['geo']['bbox']\n",
    "                        }\n",
    "                        output_data.append(new_entry)\n",
    "\n",
    "                json.dump(output_data, outfile, indent=4) \n",
    "#replace with proper input and output directory\n",
    "input_dir = 'D:/geo/geoout'  \n",
    "output_dir = 'D:/geo/processed_geooutput'  \n",
    "process_json_files(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d464533-27f9-49ea-a369-837b8c287c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "def read_json(file_path):\n",
    "    \"\"\"Reads a JSON file and returns the data.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def write_json(data, file_path):\n",
    "    \"\"\"Writes data to a JSON file.\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "#replace with proper input  directory\n",
    "directory_path = 'D:/geo/processed_geooutput'\n",
    "files = glob.glob(f'{directory_path}/*.json')\n",
    "merged_data = []\n",
    "\n",
    "for file_path in files:\n",
    "    data = read_json(file_path)\n",
    "    merged_data.extend(data)  \n",
    "\n",
    "#replace with proper output directory\n",
    "output_file_path = 'D:/geo/merged_tweets.json'\n",
    "\n",
    "write_json(merged_data, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0825ee1e-8d9a-4d46-9fcb-efeb63eb1827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "input_directory = 'D:/geo/merged_tweets.json'\n",
    "output_directory = 'D:/geo/filtered_tweets.json'\n",
    "def contains_emoji(text):\n",
    "    # Emoji ranges: https://unicode.org/emoji/charts/emoji-list.html\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.search(text) is not None\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.json'):\n",
    "        input_file_path = os.path.join(input_directory, filename)\n",
    "        output_file_path = os.path.join(output_directory, filename)\n",
    "        \n",
    "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        filtered_data = [\n",
    "            item for item in data\n",
    "            if not contains_emoji(item['text']) and\n",
    "            item['geo'] != \"Victoria, Australia\" and\n",
    "            len(item['text']) > 50 and\n",
    "            item['sentiment'] is not None\n",
    "        ]\n",
    "\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(filtered_data, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Filtered data from {filename} has been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4fd42d2-e551-417e-8dc4-54795f8208dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m expit\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:125\u001b[0m\n\u001b[0;32m    123\u001b[0m is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 125\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mkernel32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     last_error \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mget_last_error()\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import expit\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# Global initialization of model and tokenizer\n",
    "device = torch.device(\"cpu\")\n",
    "model_path = \"cardiffnlp/tweet-topic-21-multi\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "class_mapping = model.config.id2label\n",
    "\n",
    "# Simplified toot_topic_classification function\n",
    "def toot_topic_classification(toot):\n",
    "    tokens = tokenizer(toot, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
    "    output = model(**tokens)\n",
    "    scores = expit(output[\"logits\"][0].detach().numpy())\n",
    "    return [class_mapping[i] for i, prediction in enumerate((scores >= 0.5) * 1) if prediction]\n",
    "\n",
    "\n",
    "def process_json_files(input_filepath, output_filepath):\n",
    "    data = load_json(input_filepath)\n",
    "    for item in data:\n",
    "      if 'text' in item.keys():\n",
    "        text = item['text']\n",
    "        topics = toot_topic_classification(text)\n",
    "        item['topics'] = topics  # Adding the topics to the JSON object\n",
    "    save_json(data, output_filepath)\n",
    "\n",
    "# Specify the input and output file paths\n",
    "input_filepath = 'D:/geo/filtered_tweets.json'\n",
    "output_filepath = 'D:/geo/final.json'\n",
    "\n",
    "process_json_files(input_filepath, output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b5a2f-6fef-40c7-a6c3-fe75ddff83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import expit\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "def tweet_topic_classification(tweet):\n",
    "    device = torch.device(\"cpu\")  # Set to CPU to avoid issues on non-CUDA environments\n",
    "    model_path = \"cardiffnlp/tweet-topic-21-multi\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "    class_mapping = model.config.id2label\n",
    "    topics = []\n",
    "    tokens = tokenizer(tweet, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
    "    output = model(**tokens)\n",
    "    output = {key: value.to(\"cpu\") for key, value in output.items()}\n",
    "    scores = output[\"logits\"][0].detach().numpy()\n",
    "    scores = expit(scores)\n",
    "    predictions = (scores >= 0.5) * 1\n",
    "\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        if prediction:\n",
    "            topics.append(class_mapping[i])\n",
    "    return topics\n",
    "\n",
    "def process_directory(input_directory, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith('.json'):\n",
    "            input_filepath = os.path.join(input_directory, filename)\n",
    "            output_filepath = os.path.join(output_directory, filename)\n",
    "            data = load_json(input_filepath)\n",
    "            for item in data:\n",
    "                if 'text' in item:\n",
    "                    text = item['text']\n",
    "                    topics = tweet_topic_classification(text)\n",
    "                    item['topics'] = topics\n",
    "            save_json(data, output_filepath)\n",
    "            print(f\"Processed {filename} and saved to {output_filepath}\")\n",
    "\n",
    "# Specify the input and output directories\n",
    "input_directory = 'D:/geo/test2'\n",
    "output_directory = 'D:/geo/topic2'\n",
    "\n",
    "process_directory(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d72a8d9-41e4-4ee2-99ac-701decb403a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zheyu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\geopandas\\array.py:365: UserWarning: Geometry is in a geographic CRS. Results from 'sjoin_nearest' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import json\n",
    "from shapely.geometry import Point\n",
    "\n",
    "#replace with proper directory\n",
    "fp = \"D:/COMP90024_2024_ASMT2_Group12/data/SA2-Map/SA2_2021_AUST_GDA2020.shp\"\n",
    "sa2_gdf = gpd.read_file(fp)\n",
    "sa2_gdf = sa2_gdf.to_crs(epsg=4326)\n",
    "\n",
    "with open('tweet.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "#GeoDataFrame\n",
    "records = []\n",
    "for entry in data:\n",
    "    coordinates = entry['coordinates']\n",
    "    # average coordinate center\n",
    "    lon = (coordinates[0] + coordinates[2]) / 2\n",
    "    lat = (coordinates[1] + coordinates[3]) / 2\n",
    "    point = Point(lon, lat)\n",
    "    record = {\n",
    "        'id': entry['id'],\n",
    "        'created_at': entry['created_at'],\n",
    "        'sentiment': entry['sentiment'],\n",
    "        'text': entry['text'],\n",
    "        'geo': entry['geo'],\n",
    "        'topics': entry['topics'],\n",
    "        'coordinates': coordinates,\n",
    "        'geometry': point\n",
    "    }\n",
    "    records.append(record)\n",
    "\n",
    "gdf = gpd.GeoDataFrame(records, geometry='geometry', crs=\"EPSG:4326\")\n",
    "gdf_with_sa2 = gpd.sjoin_nearest(gdf, sa2_gdf, how='left', distance_col='distance')\n",
    "# suburb name\n",
    "gdf_with_sa2 = gdf_with_sa2[['id', 'created_at', 'sentiment', 'text', 'geo', 'topics', 'coordinates', 'SA2_NAME21', 'geometry']]\n",
    "\n",
    "output_data = gdf_with_sa2.drop(columns='geometry').to_dict(orient='records')\n",
    "\n",
    "with open('suburb_centre.json', 'w') as json_output_file:\n",
    "    json.dump(output_data, json_output_file, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a17d1-8eb4-4b35-8d38-bb140bcc028e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
